{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d1a951-3ba0-4856-809d-f2cbadf34eee",
   "metadata": {},
   "source": [
    "# Uncertainty Analysis using loopUI\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/gpirot/loopUI-demo/blob/main/uncertainty_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "The $loopUI$ Python package provides several functions to analyse the variability among an ensemble of voxets. It allows the computation of local measures of uncertainty (cardinality and entropy) and of global dissimilarity measures (e.g. based on multiple-point statistical analysis or wavelet decomposition among other possibilities).\n",
    "\n",
    "It is available with additional examples and notebooks on https://github.com/Loop3D/loopUI.\n",
    "\n",
    "Here we are using a few of its functions to perform uncertainty analysis. We will consider a subset of models generated by geometrical null-space navigation to compute:\n",
    "- local uncertainty indicators such as the cardinality or Shanon's entropy, as well as\n",
    "- global uncertainty indicators based on histogram dissimilarities and wavelet decomposition. \n",
    "\n",
    "### Check running environement \n",
    "and install necessary libraries if on google-colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1f61f-3712-4b6c-a2b2-d766cebabad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    !rm -rf loopUI-demo\n",
    "    !git clone https://github.com/gpirot/loopUI-demo.git\n",
    "    %cd loopUI-demo\n",
    "    !ls\n",
    "    !pip install numpy==1.26.4\n",
    "    !pip install vtk==9.3.1\n",
    "    !pip install scikit-learn\n",
    "    !pip install PyWavelets\n",
    "    !pip install loopui\n",
    "else:\n",
    "    print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9df56a-3e43-48e3-ae0d-9bb5e8082f26",
   "metadata": {},
   "source": [
    "### Importing necessary libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbda84-3eb5-40c3-b1fa-67481f938f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import loopui as ui\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# FUNCTIONS FOR OUR ANALYSIS\n",
    "from some_functions import natural_sort_key, extract_data_from_vts, plot_sections, plot_dissimilarity\n",
    "\n",
    "print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+\"IMPORTING LIBRARIES\")\n",
    "print('Using loopUI version '+ui.__version__)\n",
    "\n",
    "# PARAMETERS TO LOAD MODELS AND DISPLAY SECTIONS\n",
    "vts_folder = './models'\n",
    "required_strings = 'm_curr_'\n",
    "files='All'\n",
    "nmodels = 50 # Number of models sub-sampled in the ensemble\n",
    "ix,iy,iz = 27,34,15 # position index for sections orthogonal to x, y and z-axes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3c2ea-8376-40b8-b399-900dde45acae",
   "metadata": {},
   "source": [
    "### Listing and sorting the files available in the vts_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c59e9e-eb69-4b2b-8c9e-c5d5410023af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all .vts files\n",
    "all_vts_files = glob.glob(os.path.join(vts_folder, '*.vts'))\n",
    "\n",
    "# Filter files based on required substrings\n",
    "if required_strings:\n",
    "    filtered_files = [\n",
    "        f for f in all_vts_files if all(substring in os.path.basename(f) for substring in required_strings)\n",
    "    ]\n",
    "else:\n",
    "    filtered_files = all_vts_files\n",
    "\n",
    "# Sort naturally (e.g. file1, file2, file10)\n",
    "if files=='All':\n",
    "    vts_files = sorted(filtered_files, key=natural_sort_key)  # [::2]  # To do only 20 first plots [:20]\n",
    "else:\n",
    "    vts_files = sorted(filtered_files[::files], key=natural_sort_key)  # [::2]  # To do only 20 first plots [:20]\n",
    "\n",
    "# Display the 10 last file names\n",
    "vts_files[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30e04f-41ff-4fe7-a8bf-fbc37791b687",
   "metadata": {},
   "source": [
    "### Plotting a specific model\n",
    "<span style=\"color:red\">Exercise:</span> specify a file name in the variable `fname`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d287e1-26cf-43d7-9607-805a16d9827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8aca1-56bb-448b-b8e9-025677dbb82e",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Exercise:</span> extract the model using the `extract_data_from_vts` function by providing the file name as single argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db99710-734e-4c82-88b6-1bcf28b87a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "data_tmp = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e42ab-cace-434c-a6b0-76b832ff8d2b",
   "metadata": {},
   "source": [
    "Checking the shape of the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b764c84-21d6-4f3c-afd1-3d0f1b41658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(nz,nx,ny)=data_tmp.shape\n",
    "print(fname+\"(nz,nx,ny): \"+str(data_tmp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b98727-b76b-4daa-9eb7-b1a0c04173f5",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Exercise:</span> plot sections of the model using the `plot_sections(array,ix,iy,iz,title,label)` function by providing the following arguments:\n",
    "\n",
    "- array: numpy array of the model\n",
    "- ix: slice index on x-axis (default = 0)\n",
    "- iy: slice index on y-axis (default = 0)\n",
    "- iz: slice index on z-axis (default = 0)\n",
    "- cmap: colormap (default = 'PuRd'),more examples on https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
    "- title: of the figure (default = None)\n",
    "- label: of the displayed values for the colorbar (default = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9c189-8f18-417c-9c4a-cbf6fd830107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144e88f-93c3-497d-b25b-c8f11eca07c4",
   "metadata": {},
   "source": [
    "### Grouping the ensemble of models into a combined array for local uncertainty indicators\n",
    "When possible $loopUI$ exploits the computing abilities of $numpy$ on arrays, which avoid the use of numerous for loops.\n",
    "\n",
    "The ensemble of models is stored as a numpy array stacking the different modes along an addiional dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85749c79-334f-4de5-ab46-7938562702f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ensemble = np.zeros((nz,nx,ny,nmodels))+np.nan\n",
    "\n",
    "for i in range(nmodels):\n",
    "    fname = vts_files[i]\n",
    "    model_ensemble[:,:,:,i]= extract_data_from_vts(fname)\n",
    "    # print(fname+\" added to model_ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280153e0-edbc-456e-a3c3-dbbb97b2b961",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of the density values in the continuous domain from our ensemble of realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6104a6-18e4-47e1-ab84-ef814c3fcfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_uniqueval = len(np.unique(model_ensemble.flatten()))\n",
    "\n",
    "fig,ax = plt.subplots(1,1,dpi=100)\n",
    "plt.hist(model_ensemble.flatten(),bins=100)\n",
    "plt.text(0.5, 0.5, str(n_uniqueval)+' distinct values', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "plt.title('Histogram')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('density ($g/cm^3$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce9263-f8ce-4efb-8f6b-05d05d782641",
   "metadata": {},
   "source": [
    "Let's regroup continuous values into categories to illustrate the use of indicators on categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1408fad-41c7-4c80-ac9a-078bdc10ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=50 # Rounding precision\n",
    "model_ensemble_discrete = np.round(model_ensemble/f)*f # ensemble of categorized realizations\n",
    "np.unique(model_ensemble_discrete.flatten()) # Set of unique categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05213958-24f0-47e6-a1e5-49d421cf3c99",
   "metadata": {},
   "source": [
    "### Compute local indicators on a categorical variable\n",
    "<span style=\"color:red\">Exercise:</span> call the `entropyNcardinality` function from $loopUI$ on the ensemble of categorized realizations (single arument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade273ac-f04b-442d-9c32-bf0a077083e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "ent,crd = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd50af-7bcc-4627-b5e5-0f999ca64140",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Exercise:</span> look at the dimensions of the entropy or cardinality results and sompare them to the dimensions of a realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1af82-197b-4ad2-ba44-608f9cda9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bbbdba-fdee-410e-a91b-79462e9dbd2e",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Exercise:</span> call the `plot_sections` function to display sections of the entropy and cardinality. Adapt the title and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b43f6-a117-4a30-b230-bf28288c9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871f1c8-702b-400d-a753-f1a494977393",
   "metadata": {},
   "source": [
    "It is possible to compute the continuous entropy on ensemble of continoius variable realizations, by using the `continuous_entropy` function and specifying a number of bins for discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863ba0f-a1b4-4ff2-a190-e79a505df5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = ui.continuous_entropy(model_ensemble,nbins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e75218-9ee1-481c-a774-b83486619856",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Exercise:</span> call the `plot_sections` function to display sections of the continuous entropy, adapt the title and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1246ac-e738-4296-a230-47a6d3410f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32963671-d224-4ebe-a22b-ad6487958407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7355c11-4554-4abf-8e3a-11bb967d1fdd",
   "metadata": {},
   "source": [
    "### Compute global uncertainty indicators on an ensemble of models\n",
    "\n",
    "Now we are going to compute:\n",
    "- summary statistics for each model of the sampled ensemble (of size $nmodels$) and then \n",
    "- the dissimilarity between the summary statistics for each pair of model ( $[nmodels\\times(nmodels-1)]/2$ pairs).\n",
    "\n",
    "The first summary statistics we compute is an histogram and the dissimilarity between two histogram is estimated with the Jensen-Shanon divergence (more details here: https://github.com/Loop3D/loopUI/blob/main/ui-3-hist.ipynb).\n",
    "\n",
    "<span style=\"color:red\">Exercise:</span> complete the block of code below to call the `jsdist_hist(model_1,model_2,nbins,base)` $loopUI$ function in order to populate the dissimilarity matrix (`dist_hist`). \n",
    "\n",
    "- `model_1`: 3D array of the first model to compare\n",
    "- `model_2`: 3D array of the second model to compare\n",
    "- `nbins`: number of bins of the histogram\n",
    "- `base`: logarithmique base for the computation of the Jensen-Shanon divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f30e7-84a8-4b61-9e09-61f1771e7aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+\"COMPUTING HISTOGRAM BASED DIST ALL START\")\n",
    "\n",
    "# PARAMETERS FOR THE jsdist_hist FUNCTION\n",
    "nbins = -1\n",
    "base = 10#np.exp(1)\n",
    "\n",
    "# INITIALIZE THE DISTANCE OR DISSIMILARITY MATRIX\n",
    "dist_hist = np.zeros((nmodels,nmodels))\n",
    "\n",
    "k=0\n",
    "for i in range(nmodels):\n",
    "    for j in range(i):\n",
    "        k+=1\n",
    "        print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+'k = '+str(k)+' - i = '+str(i)+' j = ',str(j))\n",
    "        dist_hist[i,j] = ... # TO COMPLETE\n",
    "        dist_hist[j,i] = ... # TO COMPLETE\n",
    "print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+\"COMPUTING HISTOGRAM BASED DIST ALL END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2a1e8-1631-4435-84a3-7bb5e833219c",
   "metadata": {},
   "source": [
    "Now, let's plot the dissimilarity matrix and project the models in a Multi-Dimensional Scaling space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef065d8-2ddd-474b-8e91-b3ff9d5217f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1 = plot_dissimilarity(distance_mx=dist_hist,title='histogram-based dissimilarities')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfbf69-aa34-4995-a169-769bd9506e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae94364c-de3e-4cc9-b247-580dee6caeb8",
   "metadata": {},
   "source": [
    "The second summary statistics we compute are histograms obtained by wavelet decomposition and the dissimilarity between histograms is estimated with the Jensen-Shanon divergence (more details here: https://github.com/Loop3D/loopUI/blob/main/ui-7-wavelet.ipynb).\n",
    "\n",
    "<span style=\"color:red\">Exercise:</span> complete the block of code below to call the `dist_wavelet(model_1,model_2,n_levels,n_bins)` $loopUI$ function in order to populate the dissimilarity matrix (`dist_hist`). \n",
    "\n",
    "- `model_1`: 3D array of the first model to compare\n",
    "- `model_2`: 3D array of the second model to compare\n",
    "- `n_levels`: number of wavelet decompostition levels\n",
    "- `n_bins`: number of bins per histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e7ef3-c39a-4012-b41d-bed8fe4f0143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+\"COMPUTING WAVELET BASED DIST ALL START\")\n",
    "\n",
    "# PARAMETERS OF dist_wavelet THE FUNCTION\n",
    "n_levels=4\n",
    "n_bins=20\n",
    "\n",
    "# INITIALIZE THE DISTANCE OR DISSIMILARITY MATRIX\n",
    "dist_wvt = np.zeros((nmodels,nmodels))\n",
    "\n",
    "k=0\n",
    "for i in range(nmodels):\n",
    "    for j in range(i):\n",
    "        k+=1\n",
    "        print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+'k = '+str(k)+' - i = '+str(i)+' j = ',str(j))\n",
    "        dist_wvt[i,j] = ... # TO COMPLETE\n",
    "        dist_wvt[j,i] = ... # TO COMPLETE\n",
    "\n",
    "print((datetime.now()).strftime('%d-%b-%Y (%H:%M:%S)')+\" - \"+\"COMPUTING WAVELET BASED DIST ALL END\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79271ef2-0633-427e-b050-e2b81f22e543",
   "metadata": {},
   "source": [
    "Now, let's plot the dissimilarity matrix and project the models in a Multi-Dimensional Scaling space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef20bc-f7e9-44a5-b136-33923a598883",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_2 = plot_dissimilarity(distance_mx=dist_wvt,title='wavelet-based dissimilarities')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73164a10-335d-4807-a318-0774f4fcde01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
